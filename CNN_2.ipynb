{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os  # For interacting with the operating system (e.g., reading directories)\n",
    "import scipy.io  # For working with MATLAB files (.mat format)\n",
    "import numpy as np  # For handling numerical operations and arrays\n",
    "import torch  # For working with tensors, used in PyTorch\n",
    "\n",
    "# Define the sampling frequency (not used in the code, but may be useful for future data processing)\n",
    "fr_samp = 200\n",
    "\n",
    "# Specify the path to the directory containing .mat files\n",
    "path = r'./data/'  # Ensure this path points to the folder where your .mat files are stored\n",
    "\n",
    "# Get a list of all files in the specified directory\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "# Initialize empty PyTorch tensors to store data and markers\n",
    "data = torch.tensor([])  # Will hold all concatenated data from .mat files\n",
    "mark = torch.tensor([])  # Will hold all concatenated markers from .mat files\n",
    "\n",
    "# Initialize empty lists (not currently used but possibly for debugging or future use)\n",
    "data_list = []  # Could store individual data arrays if needed\n",
    "marker_list = []  # Could store individual marker arrays if needed\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for file_name in file_list:\n",
    "    # Check if the file has a .mat extension (to ensure only MATLAB files are processed)\n",
    "    if file_name.endswith('.mat'):\n",
    "        # Construct the full path to the .mat file\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        \n",
    "        # Load the .mat file as a dictionary using scipy.io.loadmat\n",
    "        mat_data = scipy.io.loadmat(file_path)\n",
    "        \n",
    "        # Extract the 'data' and 'marker' arrays from the loaded MATLAB object\n",
    "        # Assuming the structure of the MATLAB file includes 'o.data' and 'o.marker'\n",
    "        data_array = np.array(mat_data['o']['data'][0, 0])  # Convert to NumPy array\n",
    "        marker_array = np.array(mat_data['o']['marker'][0, 0])  # Convert to NumPy array\n",
    "        \n",
    "        # Convert the NumPy arrays to PyTorch tensors for further processing\n",
    "        torch_data = torch.from_numpy(data_array)\n",
    "        torch_marker = torch.from_numpy(marker_array)\n",
    "        \n",
    "        # Concatenate the current file's data and marker tensors to the overall tensors\n",
    "        # dim=0 specifies concatenation along the first axis (rows)\n",
    "        data = torch.cat((data, torch_data), dim=0)\n",
    "        mark = torch.cat((mark, torch_marker), dim=0)\n",
    "\n",
    "# Select only the first column of the 'mark' tensor\n",
    "# This assumes that the marker array has multiple columns and you need just the first one\n",
    "mark = mark[:, 0]\n",
    "\n",
    "# Print the shape of the final marker and data tensors\n",
    "print(mark.shape)  # Output the dimensions of the marker tensor for debugging\n",
    "print(data.shape)  # Output the dimensions of the data tensor for debugging\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "File Reading:\n",
    "\n",
    "The os.listdir(path) fetches all filenames in the specified directory, which are then iterated over.\n",
    "The if file_name.endswith('.mat') ensures that only MATLAB files (.mat) are processed.\n",
    "Loading .mat Files:\n",
    "\n",
    "scipy.io.loadmat() reads .mat files and loads their content into a Python dictionary. The exact structure depends on how the .mat file was created.\n",
    "Extracting Data:\n",
    "\n",
    "MATLAB objects (e.g., o.data and o.marker) are extracted using their expected paths (['o']['data'] and ['o']['marker']). Adjust this if your .mat files have a different structure.\n",
    "Tensor Concatenation:\n",
    "\n",
    "PyTorch tensors (data and mark) are initialized as empty tensors and then appended using torch.cat() during each iteration. The concatenation occurs along the first dimension (dim=0).\n",
    "Marker Column Selection:\n",
    "\n",
    "After concatenation, only the first column of the marker tensor is selected with mark[:, 0], assuming the marker array is multi-dimensional.\n",
    "Debugging Information:\n",
    "\n",
    "Printing the shapes of mark and data helps verify that the concatenation and processing are working as expected.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable to store the previous marker value\n",
    "pr = 0\n",
    "\n",
    "# Initialize empty lists to store indices for each class label\n",
    "cl_1_ind = []  # Will hold indices where the marker changes to 1\n",
    "cl_2_ind = []  # Will hold indices where the marker changes to 2\n",
    "cl_3_ind = []  # Will hold indices where the marker changes to 4\n",
    "cl_4_ind = []  # Will hold indices where the marker changes to 5\n",
    "cl_5_ind = []  # Will hold indices where the marker changes to 6\n",
    "\n",
    "# Loop through each marker in the 'mark' tensor\n",
    "for i in range(mark.shape[0]):  # mark.shape[0] gives the number of rows (or elements)\n",
    "    nx = mark[i]  # Get the current marker value\n",
    "    \n",
    "    # Check if the current marker value is different from the previous one\n",
    "    if pr != nx:\n",
    "        # Based on the new marker value, append the index to the appropriate list\n",
    "        if nx == 1:  # If the marker value changes to 1\n",
    "            cl_1_ind.append(i)\n",
    "        elif nx == 2:  # If the marker value changes to 2\n",
    "            cl_2_ind.append(i)\n",
    "        elif nx == 4:  # If the marker value changes to 4\n",
    "            cl_3_ind.append(i)\n",
    "        elif nx == 5:  # If the marker value changes to 5\n",
    "            cl_4_ind.append(i)\n",
    "        elif nx == 6:  # If the marker value changes to 6\n",
    "            cl_5_ind.append(i)\n",
    "    \n",
    "    # Update the previous marker value to the current one\n",
    "    pr = nx\n",
    "\n",
    "# Print the lengths of each class index list\n",
    "# This indicates the number of times the marker changed to each class\n",
    "print(len(cl_1_ind), len(cl_2_ind), len(cl_3_ind), len(cl_4_ind), len(cl_5_ind))\n",
    "\n",
    "\n",
    "'''\n",
    "Tracking Changes in mark:\n",
    "\n",
    "The variable pr holds the previous marker value to detect when the marker value changes during iteration.\n",
    "The condition if pr != nx ensures indices are only appended when a marker change occurs.\n",
    "Classifying Indices:\n",
    "\n",
    "Depending on the value of nx, the current index i is appended to the appropriate class list (cl_1_ind, cl_2_ind, etc.).\n",
    "This assumes that markers 1, 2, 4, 5, 6 represent distinct classes.\n",
    "Updating pr:\n",
    "\n",
    "After processing each marker, pr is updated to the current marker value (nx) to continue tracking changes.\n",
    "Counting Marker Changes:\n",
    "\n",
    "At the end of the loop, len(cl_x_ind) provides the count of indices stored for each class (e.g., how many times the marker switched to class 1, 2, etc.).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and separate trials based on marker indices\n",
    "def trial_sepration(data, mark_indx, length=fr_samp):\n",
    "    \"\"\"\n",
    "    Separates trials from the given data based on the provided marker indices.\n",
    "\n",
    "    Parameters:\n",
    "    - data: torch.Tensor\n",
    "        The input data tensor from which trials will be extracted. Assumes rows correspond to time samples and columns to features.\n",
    "    - mark_indx: list or torch.Tensor\n",
    "        A list of indices indicating where each trial begins.\n",
    "    - length: int (default=fr_samp)\n",
    "        The length (in time samples) of each trial to be extracted. Defaults to the sampling frequency.\n",
    "\n",
    "    Returns:\n",
    "    - output: torch.Tensor\n",
    "        A tensor containing all extracted trials. Each trial is of shape [length, number_of_features].\n",
    "        The output tensor will have shape [number_of_trials, length, number_of_features].\n",
    "    \"\"\"\n",
    "    # Initialize an empty tensor to store the extracted trials\n",
    "    # Shape of 'output' will grow as trials are concatenated\n",
    "    output = torch.tensor([])\n",
    "\n",
    "    # Loop through each marker index to extract trials\n",
    "    for index in mark_indx:\n",
    "        # Slice the data from the current index to index+length (a single trial)\n",
    "        # unsqueeze(0) adds a new dimension at the front to represent the trial (for stacking)\n",
    "        trial = data[index: index + length, :].unsqueeze(0)\n",
    "        \n",
    "        # Concatenate the new trial to the output tensor along the first dimension (number of trials)\n",
    "        output = torch.cat((output, trial), dim=0)\n",
    "\n",
    "    # Return the final tensor containing all extracted trials\n",
    "    return output\n",
    "\n",
    "'''\n",
    "Input Parameters:\n",
    "\n",
    "data: This is the main tensor containing the dataset. It is expected to be a 2D tensor where:\n",
    "Rows represent time samples.\n",
    "Columns represent features (e.g., channels, sensors, etc.).\n",
    "mark_indx: A list or tensor of starting indices for each trial. Each index specifies the beginning of a trial in the data.\n",
    "length: The number of samples to extract for each trial. Defaults to fr_samp, which represents the sampling frequency.\n",
    "Initialization:\n",
    "\n",
    "output: An empty tensor is initialized to store the extracted trials. This will grow dynamically as trials are appended.\n",
    "Trial Extraction:\n",
    "\n",
    "For each index in mark_indx, a slice of the data tensor is taken using data[index: index + length, :]. This extracts a trial of the specified length.\n",
    ".unsqueeze(0) adds an additional dimension at the front, changing the shape from [length, number_of_features] to [1, length, number_of_features]. This is required to stack multiple trials along the first dimension.\n",
    "Concatenation:\n",
    "\n",
    "The extracted trial is concatenated to the output tensor along the first dimension (dim=0), which represents the number of trials.\n",
    "Output:\n",
    "\n",
    "The final output tensor has the shape [number_of_trials, length, number_of_features], where:\n",
    "number_of_trials is the number of indices in mark_indx.\n",
    "length is the number of time samples per trial.\n",
    "number_of_features is the number of columns in the input data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a subset of data and corresponding markers for a specific class\n",
    "def data_mark_maker(cl_ind, new_mark, dataset=data):\n",
    "    \"\"\"\n",
    "    Creates a subset of data and corresponding markers for a given class.\n",
    "\n",
    "    Parameters:\n",
    "    - cl_ind: list or torch.Tensor\n",
    "        A list or tensor of indices indicating the starting points of trials for the specific class.\n",
    "    - new_mark: int\n",
    "        The marker value to assign to the trials of this class.\n",
    "    - dataset: torch.Tensor (default=data)\n",
    "        The input dataset from which trials will be extracted. Defaults to the global variable `data`.\n",
    "\n",
    "    Returns:\n",
    "    - cls_data: torch.Tensor\n",
    "        A tensor containing all extracted trials for the specified class. Shape: [number_of_trials, trial_length, number_of_features].\n",
    "    - cls_mark: torch.Tensor\n",
    "        A tensor containing the markers for each trial, with all values set to `new_mark`. Shape: [number_of_trials].\n",
    "    \"\"\"\n",
    "    # Extract the trials corresponding to the given class indices\n",
    "    # 'cls_data' will contain trials with shape [number_of_trials, trial_length, number_of_features]\n",
    "    cls_data = trial_sepration(dataset, cl_ind)\n",
    "    \n",
    "    # Create a tensor of the same shape as 'cls_data' filled with the new marker value\n",
    "    # torch.full creates a tensor filled with 'new_mark', and dtype=torch.long ensures integer values\n",
    "    # cls_mark is extracted as the first element of each trial, resulting in a shape [number_of_trials]\n",
    "    cls_mark = torch.full(cls_data.shape, new_mark, dtype=torch.long)[:, 0, 0]\n",
    "    \n",
    "    # Return the data tensor (cls_data) and the marker tensor (cls_mark)\n",
    "    return cls_data, cls_mark\n",
    "\n",
    "\n",
    "'''\n",
    "Input Parameters:\n",
    "\n",
    "cl_ind: A list or tensor of indices indicating the starting points of trials for a specific class. These indices are typically obtained from a marker segmentation process.\n",
    "new_mark: An integer representing the marker value to assign to all trials in this subset.\n",
    "dataset: The main dataset (data) from which trials will be extracted. Defaults to a global variable data.\n",
    "Extracting Class Data:\n",
    "\n",
    "trial_sepration(dataset, cl_ind) extracts the trials from dataset based on the indices in cl_ind. The result (cls_data) is a 3D tensor with dimensions:\n",
    "[number_of_trials, trial_length, number_of_features].\n",
    "Creating Class Markers:\n",
    "\n",
    "torch.full(cls_data.shape, new_mark, dtype=torch.long) creates a tensor with the same shape as cls_data, filled entirely with the value new_mark.\n",
    "[:, 0, 0]: This selects the first element of each trial, reducing the tensor to a 1D tensor (cls_mark) of shape [number_of_trials]. It assumes that all markers are identical for each trial, so taking the first value is sufficient.\n",
    "Output:\n",
    "\n",
    "cls_data: Contains the extracted trials for the specified class. Each trial is represented as a slice of the dataset based on the indices in cl_ind.\n",
    "cls_mark: Contains the marker values (new_mark) assigned to each trial, with one marker per trial.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data and corresponding markers for class 1\n",
    "# cl_1_ind contains indices for class 1 trials\n",
    "# Marker value assigned to class 1 trials is 0\n",
    "cls_1_data, cls_1_mark = data_mark_maker(cl_1_ind, 0)\n",
    "\n",
    "# Create data and corresponding markers for class 2\n",
    "# cl_2_ind contains indices for class 2 trials\n",
    "# Marker value assigned to class 2 trials is 1\n",
    "cls_2_data, cls_2_mark = data_mark_maker(cl_2_ind, 1)\n",
    "\n",
    "# Create data and corresponding markers for class 3\n",
    "# cl_3_ind contains indices for class 3 trials\n",
    "# Marker value assigned to class 3 trials is 2\n",
    "cls_3_data, cls_3_mark = data_mark_maker(cl_3_ind, 2)\n",
    "\n",
    "# Create data and corresponding markers for class 4\n",
    "# cl_4_ind contains indices for class 4 trials\n",
    "# Marker value assigned to class 4 trials is 3\n",
    "cls_4_data, cls_4_mark = data_mark_maker(cl_4_ind, 3)\n",
    "\n",
    "# Create data and corresponding markers for class 5\n",
    "# cl_5_ind contains indices for class 5 trials\n",
    "# Marker value assigned to class 5 trials is 4\n",
    "cls_5_data, cls_5_mark = data_mark_maker(cl_5_ind, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all class data into a single tensor\n",
    "# Combines the trials from all classes (cls_1_data, cls_2_data, ..., cls_5_data) along the first dimension (number of trials)\n",
    "# Resulting tensor 'data' will have the shape: [total_number_of_trials, trial_length, number_of_features]\n",
    "data = torch.cat((cls_1_data, cls_2_data, cls_3_data, cls_4_data, cls_5_data), dim=0)\n",
    "\n",
    "# Concatenate all class markers into a single tensor\n",
    "# Combines the markers from all classes (cls_1_mark, cls_2_mark, ..., cls_5_mark) along the first dimension\n",
    "# Resulting tensor 'mark' will have the shape: [total_number_of_trials]\n",
    "mark = torch.cat((cls_1_mark, cls_2_mark, cls_3_mark, cls_4_mark, cls_5_mark), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of samples (trials) in the 'mark' tensor\n",
    "# Since 'mark' contains the class labels for each trial, its length corresponds to the number of trials\n",
    "num_sample = mark.shape[0]\n",
    "\n",
    "# Generate a random permutation of indices from 0 to num_sample-1\n",
    "# torch.randperm generates a tensor of random indices of shape [num_sample]\n",
    "rnd_perm = torch.randperm(num_sample)\n",
    "\n",
    "# Shuffle the 'data' tensor by reordering it according to the random permutation\n",
    "# This ensures that the trials are randomly shuffled, breaking any previous order\n",
    "data = data[rnd_perm].float()\n",
    "\n",
    "# Shuffle the 'mark' tensor similarly, reordering the class labels to match the shuffled data\n",
    "# Ensuring that the class labels are still aligned with their respective trials\n",
    "mark = mark[rnd_perm].long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra dimension to the 'data' tensor at position 1\n",
    "# This is typically done to add a channel dimension (e.g., for 1D CNN models that expect a (batch, channel, length, features) format)\n",
    "data = data.unsqueeze(1)\n",
    "\n",
    "# Permute the dimensions of the 'data' tensor to change the order of axes\n",
    "# The original shape of 'data' is [num_samples, 1, trial_length, num_features]\n",
    "# We permute the dimensions to [num_samples, 1, num_features, trial_length], \n",
    "# changing the axes for trial length and number of features (e.g., for models that expect (batch, channels, features, length))\n",
    "data = data.permute(0, 1, 3, 2)\n",
    "\n",
    "# Save the 'data' tensor to a file called 'data.pth'\n",
    "# This allows you to store the processed data for later use or sharing\n",
    "torch.save(data, 'data.pth')\n",
    "\n",
    "# Save the 'mark' tensor to a file called 'mark.pth'\n",
    "# This saves the class labels (markers) corresponding to the trials in 'data'\n",
    "torch.save(mark, 'mark.pth')\n",
    "\n",
    "# Print the shapes of 'data' and 'mark' to verify the results\n",
    "print(data.shape)  # The shape of the 'data' tensor after modification\n",
    "print(mark.shape)  # The shape of the 'mark' tensor (should be [num_samples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_ratio= 0.8\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(data, mark, train_size= train_ratio)\n",
    "x_train, x_valid, y_train, y_valid= train_test_split(x_train, y_train, train_size= train_ratio)\n",
    "print('train: ', x_train.shape, y_train.shape)\n",
    "print('valid: ', x_valid.shape, y_valid.shape)\n",
    "print('test: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch sizes for training and validation/test datasets\n",
    "train_batch_size = 330  # Batch size for the training dataset\n",
    "valid_batch_size = 330  # Batch size for the validation and test datasets\n",
    "\n",
    "# Create datasets using TensorDataset, which pairs the input data and target labels\n",
    "# 'x_train', 'y_train', 'x_valid', 'y_valid', 'x_test', and 'y_test' are assumed to be pre-defined tensors\n",
    "# Each dataset contains input-output pairs, i.e., features and corresponding labels\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "# DataLoader is used to handle batching, shuffling, and loading data during model training and evaluation\n",
    "# It ensures efficient data loading and batching, especially when working with large datasets\n",
    "\n",
    "# For the training dataset, shuffle the data at the start of each epoch for randomness\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# For validation and test datasets, do not shuffle, since we want to evaluate on the full dataset without randomness\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "# Print the batch size and the number of batches in each DataLoader\n",
    "# This helps confirm that the datasets are properly batched and loaded\n",
    "print(\"train batch size:\", train_loader.batch_size, \", num of batch:\", len(train_loader))\n",
    "print(\"valid batch size:\", valid_loader.batch_size, \", num of batch:\", len(valid_loader))\n",
    "print(\"test batch size:\", test_loader.batch_size, \", num of batch:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the next batch from the training DataLoader iterator\n",
    "x, y = next(iter(train_loader))\n",
    "\n",
    "# Print the shapes of the input data (x) and the target labels (y)\n",
    "print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class= 5\n",
    "num_input= 1\n",
    "channel= 22\n",
    "signal_length= 200\n",
    "fs= 200\n",
    "\n",
    "F1= 8\n",
    "D= 3\n",
    "F2= D*F1\n",
    "\n",
    "kernel_size_1= (1,round(fs/2)) \n",
    "kernel_size_2= (channel, 1)\n",
    "kernel_size_3= (1, round(fs/8))\n",
    "kernel_size_4= (1, 1)\n",
    "\n",
    "kernel_avgpool_1= (1,4)\n",
    "kernel_avgpool_2= (1,8)\n",
    "dropout_rate= 0.2\n",
    "\n",
    "ks0= int(round((kernel_size_1[0]-1)/2))\n",
    "ks1= int(round((kernel_size_1[1]-1)/2))\n",
    "kernel_padding_1= (ks0, ks1-1)\n",
    "ks0= int(round((kernel_size_3[0]-1)/2))\n",
    "ks1= int(round((kernel_size_3[1]-1)/2))\n",
    "kernel_padding_3= (ks0, ks1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining the base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer 1: Convolutional layer with Batch Normalization\n",
    "        self.conv2d = nn.Conv2d(num_input, F1, kernel_size_1, padding=kernel_padding_1)  # 2D Convolutional layer\n",
    "        self.Batch_normalization_1 = nn.BatchNorm2d(F1)  # Batch normalization after conv\n",
    "        \n",
    "        # layer 2: Depthwise separable convolution and Batch Normalization\n",
    "        self.Depthwise_conv2D = nn.Conv2d(F1, D * F1, kernel_size_2, groups=F1)  # Depthwise convolution (groups=F1 to perform depthwise convolutions)\n",
    "        self.Batch_normalization_2 = nn.BatchNorm2d(D * F1)  # Batch normalization after depthwise conv\n",
    "        self.Elu = nn.ELU()  # Exponential Linear Unit activation function\n",
    "        self.Average_pooling2D_1 = nn.AvgPool2d(kernel_avgpool_1)  # 2D average pooling\n",
    "        self.Dropout = nn.Dropout2d(dropout_rate)  # Dropout layer (2D for spatial data)\n",
    "        \n",
    "        # layer 3: Separable convolutions (depthwise + pointwise) and Batch Normalization\n",
    "        self.Separable_conv2D_depth = nn.Conv2d(D * F1, D * F1, kernel_size_3,\n",
    "                                               padding=kernel_padding_3, groups=D * F1)  # Depthwise separable convolution\n",
    "        self.Separable_conv2D_point = nn.Conv2d(D * F1, F2, kernel_size_4)  # Pointwise convolution\n",
    "        self.Batch_normalization_3 = nn.BatchNorm2d(F2)  # Batch normalization after separable conv\n",
    "        self.Average_pooling2D_2 = nn.AvgPool2d(kernel_avgpool_2)  # 2D average pooling\n",
    "        \n",
    "        # layer 4: Fully connected (dense) layer\n",
    "        self.Flatten = nn.Flatten()  # Flatten the output to a 1D vector\n",
    "        self.Dense = nn.Linear(F2 * round(signal_length / 32), num_class)  # Dense (fully connected) layer\n",
    "        self.Softmax = nn.Softmax(dim=1)  # Softmax activation for classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # layer 1: Convolution + Batch Normalization\n",
    "        y = self.Batch_normalization_1(self.conv2d(x))  # Apply convolution and batch normalization\n",
    "        \n",
    "        # layer 2: Depthwise Convolution + ELU + Dropout + Pooling\n",
    "        y = self.Batch_normalization_2(self.Depthwise_conv2D(y))  # Apply depthwise convolution and batch normalization\n",
    "        y = self.Elu(y)  # Apply ELU activation\n",
    "        y = self.Dropout(self.Average_pooling2D_1(y))  # Apply average pooling and dropout\n",
    "        \n",
    "        # layer 3: Separable Convolution + Pointwise Convolution + ELU + Dropout + Pooling\n",
    "        y = self.Separable_conv2D_depth(y)  # Apply depthwise separable convolution\n",
    "        y = self.Batch_normalization_3(self.Separable_conv2D_point(y))  # Apply pointwise convolution and batch normalization\n",
    "        y = self.Elu(y)  # Apply ELU activation\n",
    "        y = self.Dropout(self.Average_pooling2D_2(y))  # Apply average pooling and dropout\n",
    "        \n",
    "        # layer 4: Flatten + Dense + Softmax\n",
    "        y = self.Flatten(y)  # Flatten the output to a 1D vector\n",
    "        y = self.Dense(y)  # Apply fully connected layer\n",
    "        y = self.Softmax(y)  # Apply softmax to get probabilities for classification\n",
    "        \n",
    "        return y  # Return the output (class probabilities)\n",
    "\n",
    "\n",
    "'''\n",
    "Detailed Explanation of the Model Layers:\n",
    "Layer 1: Convolution + Batch Normalization\n",
    "\n",
    "self.conv2d: A standard 2D convolutional layer with num_input input channels and F1 output channels. kernel_size_1 and padding=kernel_padding_1 define the size of the convolution filter and padding, respectively.\n",
    "self.Batch_normalization_1: Batch normalization applied after the convolution to normalize activations, helping speed up training and providing regularization.\n",
    "Layer 2: Depthwise Separable Convolution + ELU + Pooling + Dropout\n",
    "\n",
    "self.Depthwise_conv2D: A depthwise separable convolution with F1 input channels, where groups=F1 ensures each input channel is convolved separately. This significantly reduces the number of parameters.\n",
    "self.Batch_normalization_2: Batch normalization after the depthwise convolution to stabilize learning.\n",
    "self.Elu: Exponential Linear Unit (ELU) activation function applied element-wise. ELU is often used because it helps avoid dead neurons and can lead to faster convergence.\n",
    "self.Average_pooling2D_1: A 2D average pooling layer with kernel size kernel_avgpool_1 to reduce the spatial dimensions after the convolutional operations.\n",
    "self.Dropout: Dropout applied to the feature map after pooling, which helps to prevent overfitting by randomly zeroing out some of the activations.\n",
    "Layer 3: Separable Convolution + Pointwise Convolution + ELU + Pooling + Dropout\n",
    "\n",
    "self.Separable_conv2D_depth: A depthwise separable convolution, which applies a separate convolution per channel (as opposed to applying a convolution across all channels simultaneously).\n",
    "self.Separable_conv2D_point: A pointwise convolution that is used to combine the outputs of the depthwise convolution.\n",
    "This operation is typically applied after depthwise convolutions to mix the features across channels.\n",
    "self.Batch_normalization_3: Batch normalization applied after the separable convolution to normalize activations.\n",
    "self.Average_pooling2D_2: Another 2D average pooling layer, with a kernel size defined by kernel_avgpool_2.\n",
    "self.Dropout: Dropout applied after the pooling operation to further regularize the network.\n",
    "Layer 4: Fully Connected (Dense) Layer + Softmax\n",
    "\n",
    "self.Flatten: A Flatten layer is applied to convert the output from a multi-dimensional tensor (after convolution and pooling) to a 1D vector suitable for fully connected layers.\n",
    "self.Dense: A fully connected (dense) layer with F2 * round(signal_length / 32) input features (this size depends on the specific input signal length and pooling operations) and num_class output features,\n",
    "where num_class is the number of classes for classification.\n",
    "self.Softmax: The final Softmax activation layer computes the class probabilities, which will sum to 1.\n",
    "Hyperparameters:\n",
    "num_input: The number of input channels (typically 1 for EEG data).\n",
    "F1, F2: Number of filters in each layer (these are hyperparameters that you will define).\n",
    "D: A scaling factor for the number of filters in the depthwise convolution.\n",
    "kernel_size_1, kernel_size_2, kernel_size_3, kernel_size_4: Sizes of the convolution kernels for each respective convolutional layer.\n",
    "kernel_padding_1, kernel_padding_3: Padding values for each respective convolutional layer.\n",
    "kernel_avgpool_1, kernel_avgpool_2: The kernel sizes for the average pooling layers.\n",
    "dropout_rate: The dropout rate to be used in the dropout layers.\n",
    "signal_length: Length of the input signal (needed to calculate the output size after convolution and pooling).\n",
    "num_class: The number of output classes for classification.\n",
    "Why This Architecture?\n",
    "Depthwise Separable Convolutions: These convolutions (used in layer 2 and 3) are computationally efficient and reduce the number of parameters compared to standard convolutions.\n",
    "This is useful for working with small datasets like EEG signals.\n",
    "Batch Normalization: Helps stabilize training by normalizing activations, leading to faster convergence and better performance.\n",
    "Dropout: Reduces overfitting by randomly dropping neurons during training.\n",
    "Softmax: Used at the end to output probabilities for classification tasks.\n",
    "This model architecture is inspired by EEGNet, which is designed specifically for EEG signal classification. Let me know if you need further clarifications or adjustments!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the EEGNet model\n",
    "model = EEGNet()\n",
    "\n",
    "# Set the learning rate (eta) for the optimizer\n",
    "eta = 0.001\n",
    "\n",
    "# Define the loss function: Cross-Entropy Loss for classification tasks\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the optimizer (NAdam) with model parameters and learning rate\n",
    "optimizer = optim.NAdam(model.parameters(), lr=eta)\n",
    "\n",
    "'''\n",
    "Model Initialization (model = EEGNet()):\n",
    "\n",
    "This line creates an instance of the EEGNet class. This will initialize all the layers and parameters defined in your EEGNet class.\n",
    "The model is now ready to be trained.\n",
    "Learning Rate (eta = 0.001):\n",
    "\n",
    "eta is the learning rate, which controls the step size taken by the optimizer during gradient updates.\n",
    "A small learning rate like 0.001 is often a good starting point for many neural networks, though it can be adjusted based on the training results.\n",
    "Loss Function (loss_fn = nn.CrossEntropyLoss()):\n",
    "\n",
    "nn.CrossEntropyLoss() is a commonly used loss function for multi-class classification problems.\n",
    "It computes the softmax of the model’s output and then calculates the negative log-likelihood loss.\n",
    "This loss function is suitable when the model’s output consists of raw logits (not probabilities) for each class, and the targets are integer labels.\n",
    "Optimizer (optimizer = optim.NAdam(model.parameters(), lr=eta)):\n",
    "\n",
    "optim.NAdam is a variant of the Adam optimizer that uses Nesterov momentum.\n",
    "NAdam combines the benefits of Adam and Nesterov Accelerated Gradient (NAG). It has been shown to improve performance in many tasks.\n",
    "model.parameters() passes the parameters (weights and biases) of the model to the optimizer. This allows the optimizer to update these parameters during training.\n",
    "lr=eta sets the learning rate for the optimizer. This determines how large the step will be during the update of each parameter.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "def train_one_epoch(model, train_loader, loss_fn, optimizer):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize average meters to track loss and accuracy during training\n",
    "    loss_train = AverageMeter()  # A custom class to keep track of average loss\n",
    "    acc_train = Accuracy(task=\"multiclass\", num_classes=num_class)  # Accuracy metric for multiclass classification\n",
    "    \n",
    "    # Iterate over the training data loader\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass: Compute the model output (predictions)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss using the defined loss function (cross-entropy)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Backward pass: Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        # Update the model parameters based on the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the gradients after updating parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update the training loss average\n",
    "        loss_train.update(loss.item())\n",
    "        \n",
    "        # Update the accuracy metric\n",
    "        acc_train(outputs, targets.int())\n",
    "    \n",
    "    # Return the model, average loss, and accuracy for this epoch\n",
    "    return model, loss_train.avg, acc_train.compute().item()\n",
    "\n",
    "\n",
    "'''\n",
    "model.train():\n",
    "\n",
    "This sets the model to \"training mode.\" In this mode, certain layers (like dropout and batch normalization) behave differently than in evaluation mode. It ensures the model is in the correct state for training.\n",
    "loss_train = AverageMeter():\n",
    "\n",
    "AverageMeter is typically a custom class that computes the average of the loss values over the course of the epoch. It helps track the average loss without having to manually compute the mean.\n",
    "This helps you monitor the training process.\n",
    "acc_train = Accuracy(task=\"multiclass\", num_classes=num_class):\n",
    "\n",
    "Accuracy from torchmetrics is used to compute the classification accuracy. Here, it’s set for a multiclass classification task.\n",
    "num_classes=num_class defines how many classes your model is predicting.\n",
    "for i, (inputs, targets) in enumerate(train_loader)::\n",
    "\n",
    "This loop iterates through the training data (train_loader) in batches. Each batch consists of inputs (the features) and targets (the ground truth labels).\n",
    "outputs = model(inputs):\n",
    "\n",
    "This is the forward pass. The model processes the inputs and produces predictions (outputs).\n",
    "loss = loss_fn(outputs, targets):\n",
    "\n",
    "This computes the loss (error) between the model's predictions (outputs) and the ground truth labels (targets).\n",
    "loss_fn is the loss function (CrossEntropyLoss in your case), which measures how well the model's predictions align with the true labels.\n",
    "loss.backward():\n",
    "\n",
    "This computes the gradients of the loss with respect to all the model parameters (weights and biases) using backpropagation.\n",
    "nn.utils.clip_grad_norm_(model.parameters(), 1):\n",
    "\n",
    "This clips the gradients to prevent the exploding gradients problem. It ensures that the gradients do not become too large, which can destabilize the training process.\n",
    "The 1 here refers to the maximum allowable norm of the gradients (you can adjust it based on experimentation).\n",
    "optimizer.step():\n",
    "\n",
    "This updates the model’s parameters (weights and biases) using the gradients calculated in the backward pass.\n",
    "The optimizer applies an optimization algorithm (like NAdam, Adam, or SGD) to minimize the loss.\n",
    "optimizer.zero_grad():\n",
    "\n",
    "After each parameter update, this clears the gradients. In PyTorch, gradients are accumulated by default,\n",
    "so you need to manually zero them to prevent them from accumulating over multiple iterations.\n",
    "loss_train.update(loss.item()):\n",
    "This updates the running average of the loss for this epoch. loss.item() extracts the scalar value of the loss from the tensor.\n",
    "acc_train(outputs, targets.int()):\n",
    "This updates the running accuracy for the epoch. It computes the accuracy by comparing the predicted classes (outputs) with the true labels (targets).\n",
    "return model, loss_train.avg, acc_train.compute().item():\n",
    "After completing the epoch, the function returns:\n",
    "model: The updated model.\n",
    "loss_train.avg: The average loss computed over the entire epoch.\n",
    "acc_train.compute().item(): The computed accuracy for the epoch. The .item() method converts the result to a Python number (scalar).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import F1Score\n",
    "\n",
    "def test_one_epoch(model, test_loader, loss_fn, optimizer):\n",
    "    # Set the model to evaluation mode (disables dropout, batch normalization, etc.)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize average meters for tracking loss, accuracy, and F1 score\n",
    "    loss_test = AverageMeter()  # Custom class to track the average loss\n",
    "    acc_test = Accuracy(task=\"multiclass\", num_classes=num_class)  # Accuracy metric for multiclass classification\n",
    "    f1_test = F1Score(task=\"multiclass\", num_classes=num_class)  # F1 score metric for multiclass classification\n",
    "\n",
    "    # Iterate over the test data loader\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        # Forward pass: Compute the model output (predictions)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss using the loss function\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Update the loss tracker\n",
    "        loss_test.update(loss.item())\n",
    "        \n",
    "        # Update accuracy and F1 score trackers\n",
    "        acc_test(outputs, targets.int())\n",
    "        f1_test(outputs, targets.int())\n",
    "    \n",
    "    # Return the model, average loss, accuracy, and F1 score for this epoch\n",
    "    return model, loss_test.avg, acc_test.compute().item(), f1_test.compute().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize the meter, setting all values to their initial state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the values to start fresh.\"\"\"\n",
    "        # Initialize the current value, average, sum, and count\n",
    "        self.val = 0  # Current value, usually the latest value added\n",
    "        self.avg = 0  # Running average of the values\n",
    "        self.sum = 0  # Sum of all values added\n",
    "        self.count = 0  # Count of values added\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        \"\"\"Updates the meter with the latest value and count.\n",
    "\n",
    "        Args:\n",
    "            val (float): The latest value to be added.\n",
    "            n (int): The number of occurrences (used when we have multiple values for one sample, e.g., mini-batches).\n",
    "        \"\"\"\n",
    "        self.val = val  # Set the current value\n",
    "        self.sum += val * n  # Update the sum (adding val * n to account for the batch size)\n",
    "        self.count += n  # Update the count of how many values have been added\n",
    "        self.avg = self.sum / self.count  # Compute the new average value\n",
    "\n",
    "\n",
    "'''\n",
    "__init__(self):\n",
    "The constructor method initializes an instance of the AverageMeter class. When an object of this class is created,\n",
    "it immediately calls the reset() method, which sets the initial state of the attributes.\n",
    "reset(self):\n",
    "This method is used to reset all the values (val, avg, sum, and count) back to their initial states:\n",
    "val: Stores the most recent value added.\n",
    "avg: Stores the running average.\n",
    "sum: Stores the sum of all values added (helps in calculating the average).\n",
    "count: Keeps track of how many values have been added so far.\n",
    "update(self, val, n=1):\n",
    "This method is called to update the AverageMeter with new values:\n",
    "val: The latest value that needs to be added (e.g., the current loss or accuracy for a batch).\n",
    "n: The number of occurrences, usually the size of the current batch (default is 1).\n",
    "This method updates:\n",
    "val: Sets it to the latest value.\n",
    "sum: Adds the current value (val) multiplied by the batch size (n) to the running total.\n",
    "count: Increments by n to track how many values have been added.\n",
    "avg: Recalculates the running average by dividing the sum by the count.\n",
    "'''\n",
    "\n",
    "loss_train = AverageMeter()  # Initialize the average meter for tracking loss\n",
    "loss_train.update(loss.item())  # Update the meter with the current batch's loss\n",
    "print(\"Average loss for the epoch:\", loss_train.avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # Number of epochs for training the model\n",
    "\n",
    "# Lists to store the loss and accuracy history for train, validation, and test sets\n",
    "loss_train_hist, acc_train_hist = [], []\n",
    "loss_valid_hist, acc_valid_hist = [], []\n",
    "loss_test_hist, acc_test_hist = [], []\n",
    "\n",
    "# Training loop over multiple epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase: train the model for one epoch\n",
    "    model, loss_train, acc_train = train_one_epoch(model, train_loader, loss_fn, optimizer)\n",
    "    \n",
    "    # Validation phase: test the model on validation set for this epoch\n",
    "    model, loss_valid, acc_valid, _ = test_one_epoch(model, valid_loader, loss_fn, optimizer)\n",
    "    \n",
    "    # Test phase: test the model on test set for this epoch\n",
    "    model, loss_test, acc_test, f1score = test_one_epoch(model, test_loader, loss_fn, optimizer)\n",
    "  \n",
    "    # Append the loss and accuracy for the current epoch to the history lists\n",
    "    loss_train_hist.append(loss_train)\n",
    "    acc_train_hist.append(acc_train)\n",
    "    loss_valid_hist.append(loss_valid)\n",
    "    acc_valid_hist.append(acc_valid)\n",
    "    loss_test_hist.append(loss_test)\n",
    "    acc_test_hist.append(acc_test)\n",
    "    \n",
    "    # Print the loss and accuracy for each phase (train, valid, test) every 5 epochs\n",
    "    if (epoch % 5 == 0):\n",
    "        print(f'epoch {epoch}:')\n",
    "        print(f' Loss= {loss_train:.4}, Accuracy= {int(acc_train * 100)}%')  # Training loss and accuracy\n",
    "        print(f' Loss= {loss_valid:.4}, Accuracy= {int(acc_valid * 100)}%')  # Validation loss and accuracy\n",
    "        print(f' Loss= {loss_test:.4}, Accuracy= {int(acc_test * 100)}%')  # Test loss and accuracy\n",
    "        print(f' f1 score= {(f1score * 100):.4}\\n')  # Test F1 score\n",
    "\n",
    "\n",
    "'''\n",
    "num_epochs:\n",
    "This variable specifies the total number of epochs (iterations) for training the model. In your case, the model will train for 100 epochs.\n",
    "2. History Lists:\n",
    "loss_train_hist, acc_train_hist: These lists store the training loss and accuracy for each epoch.\n",
    "loss_valid_hist, acc_valid_hist: These store the validation loss and accuracy for each epoch.\n",
    "loss_test_hist, acc_test_hist: These store the test loss and accuracy for each epoch.\n",
    "3. Training Loop:\n",
    "The for loop runs for num_epochs (100 times). For each epoch:\n",
    "Training Phase (train_one_epoch): You train the model on the training dataset using the current batch of data and compute the training loss and accuracy.\n",
    "Validation Phase (test_one_epoch): After training, you evaluate the model on the validation dataset to monitor the model's performance and prevent overfitting.\n",
    "Test Phase (test_one_epoch): After validation, you evaluate the model on the test set. This gives a final measure of how well the model performs on unseen data.\n",
    "4. Appending Values to History:\n",
    "After each epoch, the loss and accuracy for train, validation, and test phases are appended to their respective lists to track the progress over time.\n",
    "5. Periodic Print:\n",
    "Every 5 epochs (epoch % 5 == 0), you print the loss and accuracy for each phase:\n",
    "Training Loss and Accuracy: loss_train and acc_train represent how well the model is fitting the training data.\n",
    "Validation Loss and Accuracy: loss_valid and acc_valid help track how the model generalizes to unseen data (validation set).\n",
    "Test Loss and Accuracy: loss_test and acc_test show the final performance on the test set after the epoch.\n",
    "F1 Score: f1score gives an additional metric to assess the model’s performance, especially in the case of imbalanced classes.\n",
    "Things to Keep in Mind:\n",
    "Model Evaluation: Every epoch, after training, you're testing the model on the validation and test sets.\n",
    "It's a good practice to monitor not just accuracy, but also other metrics like F1-score, especially if your dataset has imbalanced classes.\n",
    "Epochs: 100 epochs might be a bit high depending on the problem; you may want to tune this based on the model’s convergence behavior.\n",
    "Overfitting: If you notice the validation accuracy stagnating or decreasing while the training accuracy increases, it could be a sign of overfitting.\n",
    "Regularization techniques like dropout, early stopping, or reducing the complexity of the model could help.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting loss curves for train, validation, and test sets\n",
    "plt.plot(range(num_epochs), loss_train_hist, 'b-', label='Train')  # Blue line for training loss\n",
    "plt.plot(range(num_epochs), loss_valid_hist, 'k-', label='Valid')  # Black line for validation loss\n",
    "plt.plot(range(num_epochs), loss_test_hist, 'g-', label='Test')  # Green line for test loss\n",
    "\n",
    "# Adding labels to the plot\n",
    "plt.xlabel('Epoch')  # X-axis label: Epochs\n",
    "plt.ylabel('Loss')  # Y-axis label: Loss\n",
    "\n",
    "# Adding a grid for better visualization\n",
    "plt.grid(True)\n",
    "\n",
    "# Adding a legend to distinguish between the curves\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting accuracy curves for train, validation, and test sets\n",
    "plt.plot(range(num_epochs), acc_train_hist, 'b-', label='Train')  # Blue line for training accuracy\n",
    "plt.plot(range(num_epochs), acc_valid_hist, 'k-', label='Valid')  # Black line for validation accuracy\n",
    "plt.plot(range(num_epochs), acc_test_hist, 'g-', label='Test')  # Green line for test accuracy\n",
    "\n",
    "# Adding labels to the plot\n",
    "plt.xlabel('Epoch')  # X-axis label: Epochs\n",
    "plt.ylabel('Acc')  # Y-axis label: Accuracy\n",
    "\n",
    "# Adding a grid for better visualization\n",
    "plt.grid(True)\n",
    "\n",
    "# Adding a legend to distinguish between the curves\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
